# SQPSGBO
Source code of "SQPSGBO:Low-Cost Big Data Performance Optimization of Spark SQL Applications"

## SensQuery folder

Code to search for CSQS

### run.sh
Automatically detect sensitive queries and output the number necessary to determine the CSQ in the $TOOL_HOME/SensQuery/result/ directory
According to. The script argument is (size, num)\

For example：
$TOOL_HOME/SensQuery/run.sh 21G 30\

The above command will probe tpcds-21G for CSQ. (The finish time used below is the end of the run time. )
The output in the $TOOL_HOME/SensQuery/result/TPCDS - 21 g - finishTime folder and the config file for detection sensitivity 
query the configuration to run during the process. The runtime is the execution time corresponding to each configuration.
query's Pearson correlation coefficient.csv, query's Spearman correlation coefficient.csv, Pearson correlation high Query.txt (correlation 
A coefficient greater than 0.75 is considered highly correlated, queries joined with '&' in each row are considered highly correlated), and Spearman's correlation is high Query.txt, query configuration for execution time-related statistics.csv, query configuration for execution time.csv\
 
### calculate.py

Calling this script computes the data related to the query and outputs the data necessary to judge the sensitive query.

Parameter Description：
For example：
python3 $TOOL_HOME/SensQuery/calculate.py -b=1 -e=30 -
p=$TOOL_HOME/data/runtime/tpcds/tpcds-21G -c=$TOOL_HOME/SensQuery/result/tpcds-21G
The above command will use $TOOL_HOME/data/runtime/TPCDS/TPCDS directory - 21 g execution time data to calculate the judgment Necessary to query data, its stored in $TOOL_HOME/SensQuery/result/TPCDS - 21 g

size：data size of tpcds,like 21G、300G.
num：30 is the recommended number of Settings to run to collect the data needed to determine a CSQ.

-b：The start sequence number of the execution time file number.
-e：The end sequence number of the execution time file number.
-p：The directory in which the execution time files are stored.
-c：The resulting output directory.

### result folder
Stores data necessary to judge sensitive queries


## SQPSGBO Method

Our proposed search method

### run.sh
The automated optimization script, which will run the automated optimization and output the results to $TOOL_HOME/CSGNet-BO/config 
Directory. Script parameters are (benchmark-size, type, and defalut_runtime)\

### changeFileContent.py
Modify the timeout (in s) in the kill timeout configuration.
For example：
file_path = '$TOOL_HOME\common/errorDetection\shutDowonDetection-terasort.sh'
stop_time = 50000
changeStopTime(file_path, stop_time)
changeChmod755(file_path)\

### BO_DAF.py
The configuration parameters of a specific benchmark are optimized.

For example：python3 BO_DAF.py --benchmark=$1 --initpoints=$initNumber --
gan_initpoints=$ganinits --niters=$interationsNumber --csv_toconfig=$path/CSGNetConfig/ --
default_runtime=$3

Parameter Description：
Benchmark-size: benchmark program - dataset, e.g. wordcount-20G, redis-SS1_10
type：Benchmark program types（hibench,tpcds,tpch）
defalut_runtime：The execution time is configured by default/

### config folder

The configuration files and output files generated in the process of storing the tuning algorithm are introduced:\
dataset- It stores the similar configuration information generated by CSGNet according to the current optimal configuration after configuration failure.\
SnetConfig- All initial sample profiles produced by CSGNet.\
generationConf.csv -It stores all the sample configuration parameter information in the search process.\
logs.json- The output data of the bo algorithm, which stores information about each sample during the search process.\
output.txt- The information of failed configurations is recorded, and if this file is empty, it means that there was no failed configuration during the search\
target.png- The performance improvement process of the search sample is depicted, the abscissa corresponds to the iteration number of the sample, and the ordinate corresponds to the performance index\
confign The profile corresponding to each sample


### bayes_scode folder
BO_DAF.py Auxiliary files to call during the search\
Content Introduction:\
configuration.py -The hyperparameters used in all files are stored in this file.\
LHS_sample.py -Latin hypercube sampling is performed according to the number and range of parameters.\
bayesian_optimization.py -Bayesian optimization process function
target_space.py-The return value determines the success/failure of the current configuration, and the exploration continues if the configuration is successful, otherwise the VSGnet is used according to the optimal sample Generate similar samples and run them.\
target_space.py -Run the current exploration sample, register the sample in the hashset, and return the performance value of the sample and the execution result
Work/failure results.\
util.py- Gaussian process regression is used to implement the Bayesian optimization process, and the next exploration sample is selected according to the current existing sample space.\
event.py- Stores the current state of the bo search. Start the search, search process, end the search.\
observer.py- Observe the status of the current event and notify the logger to take action.\
logger.py -Store the sample details of the bo search process in logs.json.\
CSGNet.py -Training the neural network and generating configurations.\
model.py -Neural Network Architecture.\
Dataset.py- Processing the format of the input data to the neural network and building the training set.
